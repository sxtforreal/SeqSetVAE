"""
Optimized configuration for SeqSetVAE finetuning stage - Performance Enhanced
è§£å†³è®­ç»ƒé€Ÿåº¦æ…¢å’Œæ€§èƒ½ä¸‹é™é—®é¢˜çš„ä¼˜åŒ–é…ç½®
"""

import os
from config import *  # Import base config

# ====== Critical Performance Fixes ======

# 1. **FIXED** Learning Rate Configuration
cls_head_lr = 3e-4  # å¢åŠ åˆ†ç±»å¤´å­¦ä¹ ç‡ï¼ŒåŸæ¥1e-4å¤ªä½
lr = 1e-5  # ä¿æŒbackbone LRè¾ƒä½ï¼ˆè™½ç„¶ä¼šè¢«å†»ç»“ï¼‰

# 2. **FIXED** Batch Processing Efficiency  
batch_size = 8  # å¢åŠ batch sizeæé«˜GPUåˆ©ç”¨ç‡
gradient_accumulation_steps = 1  # å‡å°‘æ¢¯åº¦ç´¯ç§¯ï¼Œç®€åŒ–è®­ç»ƒ
num_workers = 6  # ä¼˜åŒ–æ•°æ®åŠ è½½å¹¶è¡Œåº¦

# 3. **FIXED** Training Strategy
max_epochs = 20  # å¢åŠ æœ€å¤§è®­ç»ƒè½®æ•°
early_stopping_patience = 8  # å¢åŠ æ—©åœè€å¿ƒï¼Œé¿å…è¿‡æ—©åœæ­¢
val_check_interval = 0.25  # å‡å°‘éªŒè¯é¢‘ç‡ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦

# 4. **FIXED** Loss Function Optimization
use_focal_loss = True
focal_alpha = 0.25  # å¹³è¡¡æ­£è´Ÿæ ·æœ¬
focal_gamma = 2.0  # é€‚ä¸­çš„éš¾æ ·æœ¬æƒé‡
label_smoothing = 0.0  # ç§»é™¤æ ‡ç­¾å¹³æ»‘ï¼Œé¿å…è¿‡åº¦æ­£åˆ™åŒ–

# 5. **FIXED** Regularization Settings
cls_head_weight_decay = 0.005  # å‡å°‘æƒé‡è¡°å‡
dropout_rate = 0.15  # å‡å°‘dropoutï¼Œä¿ç•™æ›´å¤šä¿¡æ¯

# 6. **FIXED** Scheduler Configuration
scheduler_type = "reduce_on_plateau"
scheduler_monitor = "val_auc"  # ç›‘æ§AUCè€Œä¸æ˜¯loss
scheduler_mode = "max"  # AUCè¶Šå¤§è¶Šå¥½
scheduler_patience = 4  # å‡å°‘è°ƒåº¦å™¨è€å¿ƒ
scheduler_factor = 0.8  # æ›´æ¸©å’Œçš„å­¦ä¹ ç‡è¡°å‡
scheduler_min_lr = 1e-6

# 7. **FIXED** Model Architecture Simplification
# ç®€åŒ–åˆ†ç±»å¤´æ¶æ„ï¼Œå‡å°‘è®¡ç®—å¼€é”€
simplified_cls_head = True
cls_head_layers = [128, 64]  # ç®€åŒ–ä¸º2å±‚
cls_dropout = 0.1  # å‡å°‘dropout

# 8. **FIXED** Training Optimizations
pin_memory = True
persistent_workers = True
prefetch_factor = 2
drop_last = True  # ä¿æŒbatchå¤§å°ä¸€è‡´æ€§

# 9. **FIXED** Monitoring Settings
monitor_metric = "val_auc"
monitor_mode = "max"
log_every_n_steps = 25  # å‡å°‘æ—¥å¿—é¢‘ç‡

# 10. **FIXED** Memory and Speed Optimizations
enable_torch_compile = True  # å¯ç”¨PyTorchç¼–è¯‘ä¼˜åŒ–
mixed_precision = True
gradient_checkpointing = False  # å¾®è°ƒæ—¶ä¸éœ€è¦æ¢¯åº¦æ£€æŸ¥ç‚¹

print("ğŸš€ Optimized Finetune Config Loaded - Performance Enhanced!")
print(f"   - Classification head LR: {cls_head_lr} (increased from 1e-4)")
print(f"   - Batch size: {batch_size} (increased for better GPU utilization)")
print(f"   - Scheduler monitors: {scheduler_monitor} (changed from val_loss)")
print(f"   - Simplified cls head: {simplified_cls_head}")
print(f"   - Early stopping patience: {early_stopping_patience}")