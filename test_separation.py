#!/usr/bin/env python3
"""
Test script to verify complete separation between pretrain and finetune models
éªŒè¯é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹å®Œå…¨åˆ†ç¦»çš„æµ‹è¯•è„šæœ¬
"""

import torch
import torch.nn as nn
from model import SeqSetVAE, SeqSetVAEPretrain
import config

def test_model_independence():
    """æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹çš„å®Œå…¨ç‹¬ç«‹æ€§"""
    print("ğŸ§ª Testing Model Independence")
    print("-" * 40)
    
    # åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹
    pretrain_model = SeqSetVAEPretrain(
        input_dim=config.input_dim,
        reduced_dim=config.reduced_dim,
        latent_dim=config.latent_dim,
        levels=config.levels,
        heads=config.heads,
        m=config.m,
        beta=config.beta,
        lr=config.lr,
        ff_dim=config.ff_dim,
        transformer_heads=config.transformer_heads,
        transformer_layers=config.transformer_layers,
    )
    
    # åˆ›å»ºå¾®è°ƒæ¨¡å‹
    finetune_model = SeqSetVAE(
        input_dim=config.input_dim,
        reduced_dim=config.reduced_dim,
        latent_dim=config.latent_dim,
        levels=config.levels,
        heads=config.heads,
        m=config.m,
        beta=config.beta,
        lr=config.lr,
        num_classes=config.num_classes,
        ff_dim=config.ff_dim,
        transformer_heads=config.transformer_heads,
        transformer_layers=config.transformer_layers,
        pretrained_ckpt=None,
    )
    
    # æ£€æŸ¥æ¨¡å‹ç±»å‹
    print(f"   - Pretrain model type: {type(pretrain_model).__name__}")
    print(f"   - Finetune model type: {type(finetune_model).__name__}")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰ä¸åŒçš„æ–¹æ³•
    pretrain_methods = set(dir(pretrain_model))
    finetune_methods = set(dir(finetune_model))
    
    finetune_only = finetune_methods - pretrain_methods
    pretrain_only = pretrain_methods - finetune_methods
    
    print(f"   - Methods only in finetune model: {len(finetune_only)}")
    if finetune_only:
        print(f"     {list(finetune_only)[:5]}...")  # Show first 5
    
    print(f"   - Methods only in pretrain model: {len(pretrain_only)}")
    if pretrain_only:
        print(f"     {list(pretrain_only)[:5]}...")  # Show first 5
    
    return True

def test_vae_feature_extraction():
    """æµ‹è¯•VAEç‰¹å¾æå–çš„ä¸åŒå®ç°"""
    print("\nğŸ§ª Testing VAE Feature Extraction Differences")
    print("-" * 40)
    
    batch_size = 2
    latent_dim = config.latent_dim
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„VAEè¾“å‡º
    mu = torch.randn(batch_size, latent_dim)
    logvar = torch.randn(batch_size, latent_dim)
    
    # æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å¾æå–ï¼ˆåº”è¯¥åªä½¿ç”¨muï¼‰
    print("   - Pretrain model feature extraction:")
    print("     Uses only mu (original design)")
    pretrain_feature = mu.clone()  # é¢„è®­ç»ƒæ¨¡å‹ç›´æ¥ä½¿ç”¨mu
    print(f"     Feature shape: {pretrain_feature.shape}")
    
    # æµ‹è¯•å¾®è°ƒæ¨¡å‹çš„ç‰¹å¾æå–ï¼ˆä½¿ç”¨mu+logvarèåˆï¼‰
    print("   - Finetune model feature extraction:")
    finetune_model = SeqSetVAE(
        input_dim=config.input_dim,
        reduced_dim=config.reduced_dim,
        latent_dim=config.latent_dim,
        levels=config.levels,
        heads=config.heads,
        m=config.m,
        beta=config.beta,
        lr=config.lr,
        num_classes=config.num_classes,
        ff_dim=config.ff_dim,
        transformer_heads=config.transformer_heads,
        transformer_layers=config.transformer_layers,
        pretrained_ckpt=None,
    )
    
    # æµ‹è¯•åˆ†ç±»æ¨¡å¼ä¸‹çš„VAEç‰¹å¾èåˆ
    finetune_model.classification_only = True
    fused_feature = finetune_model._fuse_vae_features(mu, logvar)
    print("     Uses mu + logvar fusion (enhanced design)")
    print(f"     Feature shape: {fused_feature.shape}")
    
    # éªŒè¯ç‰¹å¾ä¸åŒ
    feature_diff = torch.norm(pretrain_feature - fused_feature).item()
    print(f"     Feature difference norm: {feature_diff:.4f}")
    
    if feature_diff > 1e-6:
        print("     âœ… Features are different - models use different extraction methods")
        return True
    else:
        print("     âŒ Features are too similar - potential issue")
        return False

def test_model_parameters():
    """æµ‹è¯•æ¨¡å‹å‚æ•°ç»“æ„çš„å·®å¼‚"""
    print("\nğŸ§ª Testing Model Parameter Structures")
    print("-" * 40)
    
    # åˆ›å»ºä¸¤ä¸ªæ¨¡å‹
    pretrain_model = SeqSetVAEPretrain(
        input_dim=config.input_dim,
        reduced_dim=config.reduced_dim,
        latent_dim=config.latent_dim,
        levels=config.levels,
        heads=config.heads,
        m=config.m,
        beta=config.beta,
        lr=config.lr,
        ff_dim=config.ff_dim,
        transformer_heads=config.transformer_heads,
        transformer_layers=config.transformer_layers,
    )
    
    finetune_model = SeqSetVAE(
        input_dim=config.input_dim,
        reduced_dim=config.reduced_dim,
        latent_dim=config.latent_dim,
        levels=config.levels,
        heads=config.heads,
        m=config.m,
        beta=config.beta,
        lr=config.lr,
        num_classes=config.num_classes,
        ff_dim=config.ff_dim,
        transformer_heads=config.transformer_heads,
        transformer_layers=config.transformer_layers,
        pretrained_ckpt=None,
    )
    
    # ç»Ÿè®¡å‚æ•°
    pretrain_params = sum(p.numel() for p in pretrain_model.parameters())
    finetune_params = sum(p.numel() for p in finetune_model.parameters())
    
    print(f"   - Pretrain model parameters: {pretrain_params:,}")
    print(f"   - Finetune model parameters: {finetune_params:,}")
    print(f"   - Parameter difference: {finetune_params - pretrain_params:,}")
    
    # æ£€æŸ¥å¾®è°ƒæ¨¡å‹ç‰¹æœ‰çš„æ¨¡å—
    pretrain_modules = set(name for name, _ in pretrain_model.named_modules())
    finetune_modules = set(name for name, _ in finetune_model.named_modules())
    
    finetune_only_modules = finetune_modules - pretrain_modules
    print(f"   - Modules only in finetune model: {len(finetune_only_modules)}")
    
    # æ£€æŸ¥å…³é”®çš„å¾®è°ƒç‰¹æœ‰æ¨¡å—
    key_finetune_modules = [name for name in finetune_only_modules 
                           if any(key in name for key in ['cls_head', 'vae_feature_fusion', 'feature_fusion'])]
    
    if key_finetune_modules:
        print("     Key finetune-specific modules:")
        for module in sorted(key_finetune_modules)[:10]:  # Show first 10
            print(f"       - {module}")
        return True
    else:
        print("     âŒ No finetune-specific modules found")
        return False

def test_forward_compatibility():
    """æµ‹è¯•å‰å‘ä¼ æ’­å…¼å®¹æ€§"""
    print("\nğŸ§ª Testing Forward Pass Compatibility")
    print("-" * 40)
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 1
    seq_len = 2
    num_vars = 5
    
    sets = []
    for i in range(seq_len):
        set_dict = {
            'var': torch.randn(batch_size, num_vars, config.input_dim),
            'val': torch.rand(batch_size, num_vars, 1),
            'minute': torch.tensor([[i * 60.0]] * batch_size)
        }
        sets.append(set_dict)
    
    try:
        # æµ‹è¯•é¢„è®­ç»ƒæ¨¡å‹
        pretrain_model = SeqSetVAEPretrain(
            input_dim=config.input_dim,
            reduced_dim=config.reduced_dim,
            latent_dim=config.latent_dim,
            levels=config.levels,
            heads=config.heads,
            m=config.m,
            beta=config.beta,
            lr=config.lr,
            ff_dim=config.ff_dim,
            transformer_heads=config.transformer_heads,
            transformer_layers=config.transformer_layers,
        )
        
        pretrain_model.eval()
        with torch.no_grad():
            recon_loss, kl_loss = pretrain_model(sets)
        
        print(f"   - Pretrain model output: recon={recon_loss.item():.4f}, kl={kl_loss.item():.4f}")
        
        # æµ‹è¯•å¾®è°ƒæ¨¡å‹
        finetune_model = SeqSetVAE(
            input_dim=config.input_dim,
            reduced_dim=config.reduced_dim,
            latent_dim=config.latent_dim,
            levels=config.levels,
            heads=config.heads,
            m=config.m,
            beta=config.beta,
            lr=config.lr,
            num_classes=config.num_classes,
            ff_dim=config.ff_dim,
            transformer_heads=config.transformer_heads,
            transformer_layers=config.transformer_layers,
            pretrained_ckpt=None,
        )
        
        finetune_model.eval()
        with torch.no_grad():
            logits, recon_loss, kl_loss = finetune_model(sets)
        
        print(f"   - Finetune model output: logits={logits.shape}, recon={recon_loss.item():.4f}, kl={kl_loss.item():.4f}")
        print("   âœ… Both models can process the same input format")
        return True
        
    except Exception as e:
        print(f"   âŒ Forward pass failed: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Testing Pretrain-Finetune Separation")
    print("=" * 50)
    
    tests = [
        ("Model Independence", test_model_independence),
        ("VAE Feature Extraction", test_vae_feature_extraction),
        ("Model Parameters", test_model_parameters),
        ("Forward Compatibility", test_forward_compatibility),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"   âŒ Test failed with exception: {e}")
            results.append((test_name, False))
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("ğŸ“Š Separation Test Results:")
    passed = 0
    for test_name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   - {test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ¯ Overall: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        print("ğŸ‰ Pretrain and finetune models are completely separated!")
        print("   - Pretrain: Uses original simple design")
        print("   - Finetune: Uses enhanced design with modern VAE features")
    else:
        print("âš ï¸ Some separation issues detected.")

if __name__ == "__main__":
    main()