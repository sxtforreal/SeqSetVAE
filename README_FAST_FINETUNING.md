# Fast Classifier Head Fine-tuning with Pretrained Backbone

## üöÄ **Overview**

This script provides **fast and optimized classifier head fine-tuning** while ensuring that **all backbone parameters are properly initialized from your pretrained checkpoint**. The key improvements focus on:

1. **Guaranteed checkpoint loading** for backbone parameters
2. **Faster training** with lightweight classifier head
3. **Faster validation** with optimized feature extraction
4. **Proper parameter freezing** after checkpoint loading

## üîë **Key Features**

### **‚úì Backbone Initialization Guarantee**
- **CRITICAL**: All backbone parameters (SetVAE, Transformer, etc.) are loaded from your pretrained checkpoint
- **Classifier head**: Initialized with random weights (as intended for fine-tuning)
- **Parameter freezing**: Happens AFTER checkpoint loading to ensure proper initialization

### **‚úì Speed Optimizations**
- **Lightweight classifier**: 2-layer network (256 ‚Üí 128 ‚Üí 2) instead of 4-layer
- **Fast feature extraction**: Simple attention pooling instead of complex multi-scale fusion
- **Optimized data loading**: Increased batch size, workers, and prefetching
- **Reduced validation frequency**: Check validation less often for speed

### **‚úì Training Stability**
- **Proper weight initialization**: Xavier initialization for faster convergence
- **Cosine annealing scheduler**: Better convergence than ReduceLROnPlateau
- **Reduced gradient clipping**: 0.05 instead of 0.1 for stability

## üìã **Usage**

### **Basic Usage (Required checkpoint)**
```bash
python finetune_classifier_head_fast.py \
    --checkpoint /path/to/your/pretrained/checkpoint.ckpt
```

### **Full Usage with Custom Parameters**
```bash
python finetune_classifier_head_fast.py \
    --checkpoint /path/to/your/pretrained/checkpoint.ckpt \
    --data_dir /path/to/your/data \
    --head_lr 5e-4 \
    --batch_size 16 \
    --max_epochs 30 \
    --num_workers 8
```

### **Required Arguments**
- `--checkpoint`: **REQUIRED** - Path to your pretrained SeqSetVAE checkpoint

### **Optional Arguments**
- `--data_dir`: Data directory path (default: config default)
- `--head_lr`: Learning rate for classifier head (default: 5e-4)
- `--batch_size`: Batch size (default: 16, increased for speed)
- `--max_epochs`: Maximum training epochs (default: 30, reduced for speed)
- `--num_workers`: Data loading workers (default: 8, increased for speed)

## üèóÔ∏è **Architecture Changes**

### **Original vs. Optimized Classifier Head**

| Aspect | Original | Optimized |
|--------|----------|-----------|
| **Layers** | 4 layers (256‚Üí256‚Üí128‚Üí64‚Üí2) | 2 layers (256‚Üí128‚Üí2) |
| **Parameters** | ~99K | ~33K |
| **Feature Extraction** | Complex multi-scale fusion | Simple attention pooling |
| **Initialization** | Random | Xavier + BatchNorm |

### **Feature Extraction Comparison**

```python
# ORIGINAL: Complex multi-scale fusion (slow)
def _extract_enhanced_features(self, h_t):
    global_avg = self.feature_fusion['global_pool'](h_t.transpose(1, 2))
    global_max = self.feature_fusion['max_pool'](h_t.transpose(1, 2))
    attention_pool = self.feature_fusion['attention_pool'](query, h_t, h_t)
    combined_features = torch.cat([global_avg, global_max, attention_pool], dim=1)
    enhanced_features = self.feature_projection(combined_features)

# OPTIMIZED: Simple attention pooling (fast)
def _extract_features_fast(self, h_seq):
    query = h_seq.mean(dim=1, keepdim=True)
    attention_weights = F.softmax(torch.sum(h_seq * query, dim=-1), dim=1)
    pooled_features = torch.sum(h_seq * attention_weights.unsqueeze(-1), dim=1)
```

## ‚ö° **Speed Improvements**

### **Training Speed**
- **Batch size**: Increased from 8 to 16
- **Workers**: Increased from 4 to 8
- **Prefetching**: Enabled for faster data loading
- **Drop last**: Enabled for consistent batch sizes

### **Validation Speed**
- **Validation frequency**: Reduced from 0.2 to 0.5 (check less often)
- **Logging frequency**: Reduced from 50 to 50 steps
- **Feature extraction**: Simplified from 3 pooling methods to 1

### **Memory Efficiency**
- **Mixed precision**: 16-bit training enabled
- **Gradient accumulation**: Disabled for speed
- **Sync batch norm**: Disabled for speed
- **Deterministic**: Disabled for speed

## üîí **Parameter Management**

### **Backbone Parameters (Frozen)**
```python
# These are loaded from checkpoint and frozen:
‚úì SetVAE encoder/decoder
‚úì Transformer layers
‚úì Feature fusion modules
‚úì Positional encodings
‚úì Time encodings
```

### **Trainable Parameters (Classifier Only)**
```python
# Only these are trained:
‚úì Classifier head: 2 linear layers
‚úì BatchNorm parameters
‚úì Dropout (disabled during inference)
```

### **Parameter Freezing Process**
```python
def _freeze_backbone_after_loading(self):
    # 1. Freeze ALL parameters first
    for p in self.parameters():
        p.requires_grad = False
    
    # 2. Unfreeze ONLY classifier head
    for p in self.cls_head.parameters():
        p.requires_grad = True
    
    # 3. Put frozen modules in eval mode for speed
    self.setvae.eval()
    self.transformer.eval()
    # ... etc
```

## üìä **Expected Performance**

### **Training Speed**
- **2-3x faster** training per epoch
- **Faster convergence** due to lightweight architecture
- **Reduced memory usage** due to fewer parameters

### **Validation Speed**
- **2-4x faster** validation
- **Less frequent validation** checks
- **Faster feature extraction**

### **Model Quality**
- **Same backbone quality** (loaded from checkpoint)
- **Potentially better generalization** (less overfitting)
- **Faster adaptation** to new classification task

## üö® **Important Notes**

### **Checkpoint Requirements**
- **Must be a valid SeqSetVAE checkpoint**
- **Should contain all backbone parameters**
- **Classifier head parameters will be ignored** (as intended)

### **Data Requirements**
- **Same data format** as your pretraining
- **Compatible batch sizes** (recommend 16 or 32)
- **Sufficient workers** for your system

### **System Requirements**
- **GPU memory**: At least 8GB recommended
- **CPU cores**: At least 8 for optimal data loading
- **Storage**: Fast SSD for data loading

## üîß **Troubleshooting**

### **Common Issues**

#### **1. Checkpoint Not Found**
```bash
FileNotFoundError: Checkpoint not found: /path/to/checkpoint.ckpt
```
**Solution**: Ensure the checkpoint path is correct and file exists

#### **2. Missing Keys Warning**
```bash
Missing keys (expected for classifier): 3
```
**Solution**: This is normal - classifier head keys are expected to be missing

#### **3. CUDA Out of Memory**
```bash
RuntimeError: CUDA out of memory
```
**Solution**: Reduce batch size from 16 to 8 or 4

#### **4. Slow Training**
**Possible causes**:
- Too many workers (reduce `--num_workers`)
- Slow storage (use SSD)
- Small batch size (increase if memory allows)

### **Performance Tuning**

#### **For Faster Training**
```bash
--batch_size 32  # If memory allows
--num_workers 12  # If CPU cores allow
--max_epochs 20   # Reduce total training time
```

#### **For Better Quality**
```bash
--head_lr 3e-4   # Lower learning rate
--max_epochs 50  # More training epochs
--batch_size 8   # Smaller batches for stability
```

## üìà **Monitoring**

### **Key Metrics to Watch**
- **train_class_loss**: Should decrease steadily
- **val_auc**: Should increase over time
- **val_loss**: Should be stable or decreasing
- **Learning rate**: Should follow cosine annealing schedule

### **Expected Training Curve**
```
Epoch 1-5:   Rapid improvement in train/val loss
Epoch 5-15:  Steady improvement in validation AUC
Epoch 15-30: Fine-tuning and convergence
```

## üéØ **Best Practices**

### **1. Start with Default Settings**
```bash
python finetune_classifier_head_fast.py --checkpoint your_checkpoint.ckpt
```

### **2. Monitor Training Progress**
- Watch for overfitting (val_loss increasing while train_loss decreasing)
- Check validation AUC improvement
- Monitor learning rate schedule

### **3. Adjust Based on Results**
- **If overfitting**: Reduce learning rate or increase dropout
- **If underfitting**: Increase learning rate or training epochs
- **If slow convergence**: Check data quality and class balance

### **4. Save Best Model**
- Script automatically saves best model based on validation AUC
- Checkpoints are saved in `outputs/checkpoints/` directory
- Use `--save_top_k 3` to keep multiple checkpoints

## üìù **Summary**

This optimized script provides:

‚úÖ **Guaranteed backbone initialization** from your pretrained checkpoint  
‚úÖ **2-4x faster training and validation**  
‚úÖ **Lightweight classifier head** to prevent overfitting  
‚úÖ **Proper parameter management** (frozen backbone, trainable classifier)  
‚úÖ **Speed optimizations** throughout the pipeline  

**Start with the default settings and adjust based on your specific needs and system capabilities.**