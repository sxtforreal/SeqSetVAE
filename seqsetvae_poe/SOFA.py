#!/usr/bin/env python3
"""
Real-time SOFA score calculator per patient and per set.

This utility can:
  1) Read a wide-format CSV time series (each row = patient set) and compute SOFA
  2) Read Parquet(s) generated by seqsetvae_poe/_LVCF_SOFA.py (long format),
     convert to wide format internally, then compute SOFA

CSV wide usage:
  python SOFA.py --input input.csv --output sofa_scores.csv --mapping sofa_mapping.yaml

Parquet long (from _LVCF_SOFA.py) usage (single file or directory with *.parquet):
  python SOFA.py --input /path/to/dir_or_file --output sofa_scores.csv \
    --input_type lvcf_parquet --parquet_time_unit m

If no mapping is provided, the script expects default column names matching the
sample mapping file provided alongside this script (CSV wide case). For LVCF
parquets, the conversion uses built-in variable names from the pipeline and will
auto-build the minimal mapping needed (you can still pass --mapping to override).
"""

from __future__ import annotations

import argparse
import sys
from dataclasses import dataclass
from typing import Dict, Optional, Any, Tuple, List

import pandas as pd
import os
import glob

try:
    import yaml  # type: ignore
except Exception:  # pragma: no cover - dependency missing is handled via README
    yaml = None

# Optional progress bar (tqdm is optional dependency)
try:
    from tqdm.auto import tqdm  # type: ignore
except Exception:  # pragma: no cover - tqdm optional
    tqdm = None  # type: ignore


# ----------------------------- Data Structures ----------------------------- #


@dataclass
class ColumnMapping:
    """Container for input column names and units.

    All fields are optional; if a field is missing or None, that signal will be
    treated as unavailable and ignored in scoring for that row.
    """

    patient_id: str = "patient_id"
    set_id: Optional[str] = "set_id"
    time: str = "event_time"

    # Respiratory
    pao2_mmHg: Optional[str] = "pao2_mmHg"
    fio2_fraction: Optional[str] = "fio2_fraction"
    spo2_percent: Optional[str] = "spo2_percent"
    mech_vent: Optional[str] = "mech_vent"

    # Coagulation
    platelets_10e9_per_L: Optional[str] = "platelets_10e9_per_L"

    # Liver
    bilirubin_mg_dl: Optional[str] = "bilirubin_mg_dl"
    bilirubin_umol_L: Optional[str] = None

    # Cardiovascular
    map_mmHg: Optional[str] = "map_mmHg"
    norepinephrine_mcg_kg_min: Optional[str] = "norepinephrine_mcg_kg_min"
    epinephrine_mcg_kg_min: Optional[str] = "epinephrine_mcg_kg_min"
    dopamine_mcg_kg_min: Optional[str] = "dopamine_mcg_kg_min"
    dobutamine_mcg_kg_min: Optional[str] = "dobutamine_mcg_kg_min"

    # CNS
    gcs_total: Optional[str] = "gcs_total"

    # Renal
    creatinine_mg_dl: Optional[str] = "creatinine_mg_dl"
    creatinine_umol_L: Optional[str] = None
    urine_output_ml_24h: Optional[str] = "urine_output_ml_24h"


def load_mapping(path: Optional[str]) -> ColumnMapping:
    """Load a ColumnMapping from YAML file or return defaults when None.

    The YAML structure should be flat, using keys that match ColumnMapping fields.
    """

    if path is None:
        return ColumnMapping()

    if yaml is None:
        raise RuntimeError(
            "PyYAML is required to load a mapping file. Install via requirements.txt"
        )

    with open(path, "r", encoding="utf-8") as f:
        data: Dict[str, Any] = yaml.safe_load(f) or {}

    # Validate keys against ColumnMapping fields
    valid_keys = set(ColumnMapping.__annotations__.keys())
    unknown_keys = [k for k in data.keys() if k not in valid_keys]
    if unknown_keys:
        raise ValueError(
            f"Unknown mapping keys: {unknown_keys}. Valid keys: {sorted(valid_keys)}"
        )

    return ColumnMapping(**data)


# ------------------------------ Helper Utils ------------------------------- #


def to_float(value: Any) -> Optional[float]:
    """Best-effort conversion to float, returns None on failure or NaN.

    Handles strings, numbers, and pandas NA types gracefully.
    """

    if value is None:
        return None
    try:
        f = float(value)
    except Exception:
        return None
    if pd.isna(f):
        return None
    return f


def get_value(row: pd.Series, col: Optional[str]) -> Optional[float]:
    if col is None:
        return None
    if col not in row:
        return None
    return to_float(row[col])


def convert_bilirubin_to_mg_dl(
    bilirubin_mg_dl: Optional[float], bilirubin_umol_L: Optional[float]
) -> Optional[float]:
    """Return bilirubin in mg/dL, converting from umol/L when needed.

    Conversion: 1 mg/dL = 17.104 umol/L
    """

    if bilirubin_mg_dl is not None:
        return bilirubin_mg_dl
    if bilirubin_umol_L is None:
        return None
    return bilirubin_umol_L / 17.104


def convert_creatinine_to_mg_dl(
    creatinine_mg_dl: Optional[float], creatinine_umol_L: Optional[float]
) -> Optional[float]:
    """Return creatinine in mg/dL, converting from umol/L when needed.

    Conversion: 1 mg/dL = 88.4 umol/L
    """

    if creatinine_mg_dl is not None:
        return creatinine_mg_dl
    if creatinine_umol_L is None:
        return None
    return creatinine_umol_L / 88.4


def compute_resp_subscore(row: pd.Series, m: ColumnMapping) -> Optional[int]:
    pao2 = get_value(row, m.pao2_mmHg)
    fio2 = get_value(row, m.fio2_fraction)
    spo2 = get_value(row, m.spo2_percent)
    mech_vent_flag = get_value(row, m.mech_vent)
    on_vent = bool(mech_vent_flag) if mech_vent_flag is not None else False

    pf_ratio: Optional[float] = None
    if pao2 is not None and fio2 is not None and fio2 > 0:
        pf_ratio = pao2 / fio2

    # If PF not available, attempt SF ratio approximation
    sf_ratio: Optional[float] = None
    if pf_ratio is None and spo2 is not None and fio2 is not None and fio2 > 0:
        # SpO2 is expected as percent (e.g., 95); FiO2 as fraction (e.g., 0.4)
        sf_ratio = spo2 / fio2

    score: Optional[int] = None
    # Prioritize PF ratio when available
    if pf_ratio is not None:
        if on_vent and pf_ratio < 100:
            score = 4
        elif on_vent and pf_ratio < 200:
            score = 3
        elif pf_ratio < 300:
            score = 2
        elif pf_ratio < 400:
            score = 1
        else:
            score = 0
        return score

    # Approximate using SF ratio if PF unavailable
    if sf_ratio is not None:
        # Approximate thresholds per literature: SF ~ 235 ~ PF 200; SF ~ 315 ~ PF 300
        if on_vent and sf_ratio < 150:
            return 4
        if on_vent and sf_ratio < 235:
            return 3
        if sf_ratio < 315:
            return 2
        if sf_ratio < 400:
            return 1
        return 0

    return None


def compute_coag_subscore(row: pd.Series, m: ColumnMapping) -> Optional[int]:
    platelets = get_value(row, m.platelets_10e9_per_L)
    if platelets is None:
        return None
    if platelets < 20:
        return 4
    if platelets < 50:
        return 3
    if platelets < 100:
        return 2
    if platelets < 150:
        return 1
    return 0


def compute_liver_subscore(row: pd.Series, m: ColumnMapping) -> Optional[int]:
    bilirubin = convert_bilirubin_to_mg_dl(
        get_value(row, m.bilirubin_mg_dl), get_value(row, m.bilirubin_umol_L)
    )
    if bilirubin is None:
        return None
    if bilirubin >= 12.0:
        return 4
    if bilirubin >= 6.0:
        return 3
    if bilirubin >= 2.0:
        return 2
    if bilirubin >= 1.2:
        return 1
    return 0


def compute_cardiovascular_subscore(row: pd.Series, m: ColumnMapping) -> Optional[int]:
    map_value = get_value(row, m.map_mmHg)
    norepi = get_value(row, m.norepinephrine_mcg_kg_min) or 0.0
    epi = get_value(row, m.epinephrine_mcg_kg_min) or 0.0
    dopa = get_value(row, m.dopamine_mcg_kg_min) or 0.0
    dobut = get_value(row, m.dobutamine_mcg_kg_min) or 0.0

    # Vasopressor-based grading has priority over MAP-only grading
    if norepi > 0.1 or epi > 0.1 or dopa > 15:
        return 4
    if (0.0 < norepi <= 0.1) or (0.0 < epi <= 0.1) or (5 < dopa <= 15):
        return 3
    if (0.0 < dopa <= 5) or (dobut > 0.0):
        return 2

    if map_value is None:
        return None
    if map_value < 70:
        return 1
    return 0


def compute_cns_subscore(row: pd.Series, m: ColumnMapping) -> Optional[int]:
    gcs = get_value(row, m.gcs_total)
    if gcs is None:
        return None
    if gcs < 6:
        return 4
    if gcs <= 9:
        return 3
    if gcs <= 12:
        return 2
    if gcs <= 14:
        return 1
    return 0


def compute_renal_subscore(row: pd.Series, m: ColumnMapping) -> Optional[int]:
    creat = convert_creatinine_to_mg_dl(
        get_value(row, m.creatinine_mg_dl), get_value(row, m.creatinine_umol_L)
    )
    urine_24h = get_value(row, m.urine_output_ml_24h)

    creat_score: Optional[int] = None
    if creat is not None:
        if creat >= 5.0:
            creat_score = 4
        elif creat >= 3.5:
            creat_score = 3
        elif creat >= 2.0:
            creat_score = 2
        elif creat >= 1.2:
            creat_score = 1
        else:
            creat_score = 0

    urine_score: Optional[int] = None
    if urine_24h is not None:
        if urine_24h < 200:
            urine_score = 4
        elif urine_24h < 500:
            urine_score = 3
        else:
            urine_score = 0

    if creat_score is None and urine_score is None:
        return None
    if creat_score is None:
        return urine_score
    if urine_score is None:
        return creat_score
    return max(creat_score, urine_score)


def compute_row_scores(row: pd.Series, m: ColumnMapping) -> Tuple[Optional[int], Optional[int], Optional[int], Optional[int], Optional[int], Optional[int]]:
    return (
        compute_resp_subscore(row, m),
        compute_coag_subscore(row, m),
        compute_liver_subscore(row, m),
        compute_cardiovascular_subscore(row, m),
        compute_cns_subscore(row, m),
        compute_renal_subscore(row, m),
    )


def forward_fill_by_patient(df: pd.DataFrame, patient_col: str) -> pd.DataFrame:
    # Forward-fill all columns per patient to simulate real-time availability
    filled = (
        df.sort_values([patient_col, "__event_time__"])  # type: ignore[index]
        .groupby(patient_col, as_index=False, sort=False)
        .apply(lambda g: g.ffill())
        .reset_index(drop=True)
    )
    return filled


def compute_sofa(df: pd.DataFrame, mapping: ColumnMapping) -> pd.DataFrame:
    # Prepare time column
    if mapping.time not in df.columns:
        raise ValueError(f"Time column '{mapping.time}' not found in input data")
    df = df.copy()
    df["__event_time__"] = pd.to_datetime(df[mapping.time], errors="coerce")
    if df["__event_time__"].isna().any():
        raise ValueError("Some rows have invalid timestamps in time column")

    # Sort and forward-fill per patient
    df = df.sort_values([mapping.patient_id, "__event_time__"]).reset_index(drop=True)
    df = forward_fill_by_patient(df, mapping.patient_id)

    # Compute subscores with optional progress bar
    if tqdm is not None and hasattr(tqdm, "pandas"):
        try:
            tqdm.pandas(desc="Computing SOFA subscores")
            subscores = df.progress_apply(
                lambda row: compute_row_scores(row, mapping),
                axis=1,
                result_type="expand",
            )
        except Exception:
            subscores = df.apply(
                lambda row: compute_row_scores(row, mapping),
                axis=1,
                result_type="expand",
            )
    else:
        subscores = df.apply(
            lambda row: compute_row_scores(row, mapping),
            axis=1,
            result_type="expand",
        )
    subscores.columns = [
        "respiratory_score",
        "coagulation_score",
        "liver_score",
        "cardiovascular_score",
        "cns_score",
        "renal_score",
    ]

    out = pd.concat([df[[mapping.patient_id, mapping.time] + ([mapping.set_id] if mapping.set_id and mapping.set_id in df.columns else [])].rename(columns={mapping.time: "event_time"}), subscores], axis=1)

    # Total score: sum of available subscores
    out["sofa_total"] = out[[
        "respiratory_score",
        "coagulation_score",
        "liver_score",
        "cardiovascular_score",
        "cns_score",
        "renal_score",
    ]].sum(axis=1, min_count=1)

    # Ensure integer dtype where possible
    for col in [
        "respiratory_score",
        "coagulation_score",
        "liver_score",
        "cardiovascular_score",
        "cns_score",
        "renal_score",
        "sofa_total",
    ]:
        out[col] = out[col].astype("Int64")  # pandas nullable integer

    # Order columns
    base_cols = [mapping.patient_id]
    if mapping.set_id and mapping.set_id in out.columns:
        base_cols.append(mapping.set_id)
    base_cols.append("event_time")

    score_cols = [
        "respiratory_score",
        "coagulation_score",
        "liver_score",
        "cardiovascular_score",
        "cns_score",
        "renal_score",
        "sofa_total",
    ]

    out = out[base_cols + score_cols]
    return out


# --------------------------- LVCF Parquet Conversion --------------------------- #


def _build_wide_from_lvcf_parquets(paths: List[str], *, time_unit: str = "m") -> pd.DataFrame:
    """Convert long-format Parquet(s) from _LVCF_SOFA.py to a single wide DataFrame.

    Each Parquet is per-patient with columns: [variable, value, time, set_index, is_carry, v0..]
    We pivot per set_index, attach event_time from max(time) within the set, infer patient_id
    from the filename stem, and compute gcs_total from GCS_eye/motor/verbal when present.

    Args:
        paths: List of Parquet file paths (each one a single patient)
        time_unit: Pandas to_datetime unit for the numeric "time" column (default minutes)

    Returns:
        Wide DataFrame with at least columns: [patient_id, set_index, event_time]
        and columns for SOFA variables available.
    """
    wides: List[pd.DataFrame] = []
    file_iter = (
        tqdm(paths, desc="Converting LVCF to wide", unit="file") if tqdm is not None else paths
    )
    for fp in file_iter:
        try:
            df = pd.read_parquet(fp, engine="pyarrow")
        except Exception as exc:
            raise RuntimeError(f"Failed to read parquet '{fp}': {exc}")

        for req in ["variable", "value", "time", "set_index"]:
            if req not in df.columns:
                raise ValueError(f"Input parquet '{fp}' missing required column '{req}' from _LVCF_SOFA.py output")

        # Long -> wide per set_index
        val_wide = (
            df.pivot_table(
                index="set_index", columns="variable", values="value", aggfunc="last"
            )
            .reset_index()
        )
        time_df = (
            df.groupby("set_index", as_index=False)["time"].max().rename(columns={"time": "event_time"})
        )
        wide = val_wide.merge(time_df, on="set_index", how="left")

        # Construct event_time; if time is relative minutes, map to a dummy timeline
        try:
            wide["event_time"] = pd.to_datetime(wide["event_time"], unit=time_unit, origin="unix")
        except Exception:
            # Fallback: treat as absolute string
            wide["event_time"] = pd.to_datetime(wide["event_time"], errors="coerce")

        # Derive patient_id from filename
        pid = os.path.splitext(os.path.basename(fp))[0]
        wide["patient_id"] = pid

        # Compute GCS total if components exist
        for c in ["GCS_eye", "GCS_motor", "GCS_verbal"]:
            if c not in wide.columns:
                wide[c] = pd.NA
        wide["gcs_total"] = wide[["GCS_eye", "GCS_motor", "GCS_verbal"]].sum(axis=1, min_count=1)

        # Ensure ordering columns present
        if "set_index" not in wide.columns:
            raise ValueError(f"Wide conversion missing 'set_index' for '{fp}'")

        wides.append(wide)

    if len(wides) == 0:
        return pd.DataFrame(columns=["patient_id", "set_index", "event_time"])  # empty
    out = pd.concat(wides, axis=0, ignore_index=True)
    return out


def parse_args(argv: Optional[list[str]] = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Compute real-time SOFA score per patient and per set")
    parser.add_argument("--input", required=True, help="Path to input file or directory")
    parser.add_argument("--output", required=True, help="Path to write output CSV with SOFA scores")
    parser.add_argument(
        "--input_type",
        choices=["csv", "lvcf_parquet"],
        default="csv",
        help="Input format: 'csv' for wide CSV; 'lvcf_parquet' for _LVCF_SOFA.py outputs (file or dir)",
    )
    parser.add_argument(
        "--mapping",
        default=None,
        help="Optional YAML file mapping input column names to expected fields. See sofa_mapping.example.yaml",
    )
    # CSV options
    parser.add_argument("--delimiter", default=",", help="CSV delimiter (default ',')")
    parser.add_argument("--encoding", default="utf-8", help="CSV encoding (default utf-8)")
    # Parquet options
    parser.add_argument(
        "--parquet_time_unit",
        default="m",
        help="Unit for numeric 'time' in LVCF Parquets when converting to datetime (e.g., 'm' minutes, 's' seconds)",
    )
    return parser.parse_args(argv)


def _collect_parquet_files_for_all_splits(input_path: str) -> List[str]:
    """Collect parquet files ensuring all train/valid/test splits are included.

    If the provided path points to one of the split directories (basename is one
    of {train, valid, test}), this will also search the sibling split
    directories under the same parent so that all patient parquet files across
    splits are included. Otherwise, it searches recursively within the provided
    directory. If a single parquet file is provided, it is returned as-is.
    """

    # Single file
    if os.path.isfile(input_path):
        return [input_path] if input_path.lower().endswith(".parquet") else []

    inputs: List[str] = []
    if os.path.isdir(input_path):
        base = os.path.basename(os.path.normpath(input_path))
        parent = os.path.dirname(os.path.normpath(input_path))
        split_names = ["train", "valid", "test"]

        # If pointing at a split dir, include siblings under the same parent
        if base in split_names and os.path.isdir(parent):
            for split in split_names:
                split_dir = os.path.join(parent, split)
                if os.path.isdir(split_dir):
                    pattern_recursive = os.path.join(split_dir, "**", "*.parquet")
                    pattern_flat = os.path.join(split_dir, "*.parquet")
                    inputs.extend(glob.glob(pattern_recursive, recursive=True))
                    inputs.extend(glob.glob(pattern_flat))
        else:
            # Default: search within the provided directory
            pattern_recursive = os.path.join(input_path, "**", "*.parquet")
            pattern_flat = os.path.join(input_path, "*.parquet")
            inputs.extend(glob.glob(pattern_recursive, recursive=True))
            inputs.extend(glob.glob(pattern_flat))

    # Deduplicate and sort for stability
    inputs = sorted(set(inputs))
    return inputs


def main(argv: Optional[list[str]] = None) -> int:
    args = parse_args(argv)
    # Branch: LVCF Parquet long-format vs CSV wide-format
    if args.input_type == "lvcf_parquet":
        # Collect parquet files ensuring all split directories are covered
        inputs: List[str] = _collect_parquet_files_for_all_splits(args.input)
        if not inputs:
            print("No parquet files found for LVCF conversion.", file=sys.stderr)
            return 2

        # Convert to wide
        try:
            wide = _build_wide_from_lvcf_parquets(inputs, time_unit=args.parquet_time_unit)
        except Exception as exc:
            print(f"Failed to convert LVCF Parquets to wide format: {exc}", file=sys.stderr)
            return 2

        # Build default mapping for LVCF unless provided
        if args.mapping is not None:
            mapping = load_mapping(args.mapping)
        else:
            mapping = ColumnMapping(
                patient_id="patient_id",
                set_id="set_index",
                time="event_time",
                # Respiratory
                pao2_mmHg="PO2",
                fio2_fraction="FiO2",
                spo2_percent=None,
                mech_vent="Intubated",
                # Coagulation
                platelets_10e9_per_L="Platelet Count",
                # Liver
                bilirubin_mg_dl="Bilirubin (Total)",
                bilirubin_umol_L=None,
                # Cardiovascular
                map_mmHg="MBP",
                norepinephrine_mcg_kg_min="Norepinephrine",
                epinephrine_mcg_kg_min="Epinephrine",
                dopamine_mcg_kg_min="Dopamine",
                dobutamine_mcg_kg_min=None,
                # CNS
                gcs_total="gcs_total",
                # Renal
                creatinine_mg_dl="Creatinine Blood",
                creatinine_umol_L=None,
                urine_output_ml_24h=None,
            )

        # Compute
        try:
            result = compute_sofa(wide, mapping)
        except Exception as exc:
            print(f"SOFA computation failed: {exc}", file=sys.stderr)
            return 4

    else:
        # CSV wide-format path (backward compatible)
        mapping = load_mapping(args.mapping)
        try:
            df = pd.read_csv(args.input, delimiter=args.delimiter, encoding=args.encoding)
        except Exception as exc:
            print(f"Failed to read input CSV: {exc}", file=sys.stderr)
            return 2
        required_base_cols = [mapping.patient_id, mapping.time]
        for col in required_base_cols:
            if col not in df.columns:
                print(f"Missing required column '{col}' in input CSV", file=sys.stderr)
                return 3
        try:
            result = compute_sofa(df, mapping)
        except Exception as exc:
            print(f"SOFA computation failed: {exc}", file=sys.stderr)
            return 4

    # Write output
    try:
        result.to_csv(args.output, index=False)
    except Exception as exc:
        print(f"Failed to write output CSV: {exc}", file=sys.stderr)
        return 5

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
