# SeqSetVAE: å…¨è®­ç»ƒæ¨¡å¼ vs åˆ†ç±»æ¨¡å¼è¯¦ç»†å¯¹æ¯”

## ğŸ” æ¨¡å¼æ¦‚è¿°

| ç‰¹æ€§ | å…¨è®­ç»ƒæ¨¡å¼ (Full Training) | åˆ†ç±»æ¨¡å¼ (Classification-Only) |
|------|---------------------------|--------------------------------|
| **ä½¿ç”¨åœºæ™¯** | é¢„è®­ç»ƒé˜¶æ®µ | å¾®è°ƒé˜¶æ®µ |
| **æ ‡å¿—ä½** | `self.classification_only = False` | `self.classification_only = True` |
| **ä¸»è¦ç›®æ ‡** | å­¦ä¹ è‰¯å¥½çš„è¡¨ç¤º | ä¼˜åŒ–åˆ†ç±»æ€§èƒ½ |

## ğŸ“Š æ ¸å¿ƒåŒºåˆ«è¯¦è§£

### 1. **æŸå¤±å‡½æ•°è®¡ç®—**

#### ğŸ”„ å…¨è®­ç»ƒæ¨¡å¼
```python
# è®¡ç®—å®Œæ•´çš„ELBOæŸå¤±
if self.classification_only:
    # ä¸ä¼šæ‰§è¡Œè¿™ä¸ªåˆ†æ”¯
else:
    # è®¡ç®—é‡å»ºæŸå¤±
    recon_loss_total = 0.0
    for idx, s_dict in enumerate(sets):
        recon = self.decoder(h_seq[:, idx], N_t, noise_std=0.3)
        recon_loss_total += chamfer_recon_loss(recon, target_x)
    
    # æ€»æŸå¤± = é¢„æµ‹æŸå¤± + é‡å»ºæŸå¤± + KLæŸå¤±
    total_loss = pred_weight * pred_loss + recon_weight * recon_loss + kl_loss
```

#### ğŸ¯ åˆ†ç±»æ¨¡å¼
```python
if self.classification_only:
    # è·³è¿‡é‡å»ºï¼Œåªè®¡ç®—åˆ†ç±»æŸå¤±
    total_loss = pred_loss
    recon_loss = torch.tensor(0.0)  # è®¾ä¸º0
    kl_loss = torch.tensor(0.0)     # è®¾ä¸º0
```

### 2. **ç‰¹å¾æå–ç­–ç•¥**

#### ğŸ”„ å…¨è®­ç»ƒæ¨¡å¼ - å¤æ‚å¤šå°ºåº¦ç‰¹å¾æå–
```python
if self.classification_only:
    # ä¸ä¼šæ‰§è¡Œè¿™ä¸ªåˆ†æ”¯
else:
    # ä½¿ç”¨å¤æ‚çš„å¤šå°ºåº¦pooling
    global_avg = self.feature_fusion['global_pool'](h_t.transpose(1, 2))
    global_max = self.feature_fusion['max_pool'](h_t.transpose(1, 2))
    
    # æ³¨æ„åŠ›pooling
    attn_output, _ = self.feature_fusion['attention_pool'](query, h_t, h_t)
    
    # èåˆæ‰€æœ‰ç‰¹å¾
    combined_features = torch.cat([global_avg, global_max, attention_pool], dim=1)
    enhanced_features = self.feature_projection(combined_features)
```

#### ğŸ¯ åˆ†ç±»æ¨¡å¼ - ç®€å•ç¨³å®šç‰¹å¾æå–
```python
if self.classification_only:
    # ä½¿ç”¨æœ€æ–°æ—¶åˆ»çš„ç‰¹å¾ + æ³¨æ„åŠ›åŠ æƒ
    last_token = h_t[:, -1, :]  # æœ€è¿‘çš„è¡¨ç¤º
    
    # ç®€å•çš„æ³¨æ„åŠ›åŠ æƒpooling
    attn_weights = F.softmax(
        torch.matmul(h_t, last_token.unsqueeze(-1)).squeeze(-1), dim=1
    )
    attn_pooled = torch.sum(h_t * attn_weights.unsqueeze(-1), dim=1)
    
    # ç®€å•ç»„åˆï¼š70%æœ€æ–° + 30%æ³¨æ„åŠ›åŠ æƒ
    enhanced_features = 0.7 * last_token + 0.3 * attn_pooled
```

### 3. **VAEç‰¹å¾èåˆç­–ç•¥**

#### ğŸ”„ å…¨è®­ç»ƒæ¨¡å¼ - å¯å­¦ä¹ é—¨æ§èåˆ
```python
if hasattr(self, 'vae_feature_fusion') and not self.classification_only:
    # å¤æ‚çš„å¯å­¦ä¹ èåˆ
    mu_proj = self.vae_feature_fusion['mean_projection'](mu)
    var_proj = self.vae_feature_fusion['var_projection'](std)
    
    # å­¦ä¹ èåˆæƒé‡
    fusion_gate = self.vae_feature_fusion['fusion_gate'](
        torch.cat([mu_proj, var_proj], dim=-1)
    )
    
    # é—¨æ§ç»„åˆ
    fused = fusion_gate * mu_proj + (1 - fusion_gate) * var_proj
    
    # ä¸ç¡®å®šæ€§æ ¡å‡†
    uncertainty_score = self.vae_feature_fusion['uncertainty_calibration'](std)
    fused = fused * (1.0 + 0.1 * uncertainty_score)
```

#### ğŸ¯ åˆ†ç±»æ¨¡å¼ - ç®€å•ä¸ç¡®å®šæ€§åŠ æƒ
```python
else:  # classification_only = True
    # ç®€å•ä½†æœ‰æ•ˆçš„èåˆ
    uncertainty = torch.mean(std, dim=-1, keepdim=True)
    uncertainty_weight = torch.sigmoid(-uncertainty + 1.0)
    
    # æ–¹å·®è°ƒåˆ¶å‡å€¼ç‰¹å¾
    variance_modulation = 1.0 + 0.05 * torch.tanh(std)
    modulated_mu = mu * variance_modulation
    
    # ä¸ç¡®å®šæ€§åŠ æƒç»„åˆ
    fused = uncertainty_weight * modulated_mu + (1 - uncertainty_weight) * mu
```

### 4. **ä¼˜åŒ–å™¨é…ç½®**

#### ğŸ”„ å…¨è®­ç»ƒæ¨¡å¼
```python
else:  # not classification_only
    # å¤šç»„å‚æ•°ï¼Œä¸åŒå­¦ä¹ ç‡
    setvae_params = list(self.setvae.parameters())
    transformer_params = list(self.transformer.parameters())
    cls_params = list(self.cls_head.parameters())
    
    optimizer = AdamW([
        {'params': setvae_params, 'lr': self.lr * 0.5},
        {'params': transformer_params, 'lr': self.lr},
        {'params': cls_params, 'lr': self.lr * 2.0}
    ])
```

#### ğŸ¯ åˆ†ç±»æ¨¡å¼
```python
if self.classification_only:
    # åªä¼˜åŒ–åˆ†ç±»å¤´
    cls_params = [p for p in self.cls_head.parameters() if p.requires_grad]
    optimizer = AdamW(
        [{'params': cls_params, 'lr': cls_lr}],
        weight_decay=0.01  # æ›´å¼ºçš„æ­£åˆ™åŒ–
    )
```

### 5. **æ¨¡å‹çŠ¶æ€ç®¡ç†**

#### ğŸ”„ å…¨è®­ç»ƒæ¨¡å¼
- æ‰€æœ‰æ¨¡å—éƒ½åœ¨è®­ç»ƒæ¨¡å¼
- å…è®¸dropoutã€batch normç­‰éšæœºæ€§
- å‚æ•°æ›´æ–°å½±å“æ•´ä¸ªç½‘ç»œ

#### ğŸ¯ åˆ†ç±»æ¨¡å¼
```python
def set_backbone_eval(self):
    # å¼ºåˆ¶backboneä¸ºevalæ¨¡å¼
    self.setvae.eval()
    self.transformer.eval()
    self.post_transformer_norm.eval()
    self.decoder.eval()
    self.feature_fusion.eval()
    self.vae_feature_fusion.eval()

def on_train_start(self):
    if self.classification_only:
        self.set_backbone_eval()  # æ¯æ¬¡è®­ç»ƒå¼€å§‹æ—¶å¼ºåˆ¶eval
```

## ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦ä¸¤ç§æ¨¡å¼ï¼Ÿ

### å…¨è®­ç»ƒæ¨¡å¼çš„ä¼˜åŠ¿
1. **è¡¨ç¤ºå­¦ä¹ **: é€šè¿‡é‡å»ºä»»åŠ¡å­¦ä¹ æœ‰æ„ä¹‰çš„æ½œåœ¨è¡¨ç¤º
2. **æ­£åˆ™åŒ–**: KLæŸå¤±é˜²æ­¢åéªŒåå¡Œ
3. **æ³›åŒ–èƒ½åŠ›**: å¤šä»»åŠ¡å­¦ä¹ æé«˜æ³›åŒ–æ€§èƒ½

### åˆ†ç±»æ¨¡å¼çš„ä¼˜åŠ¿
1. **ç¨³å®šæ€§**: å†»ç»“çš„backboneæä¾›ç¨³å®šçš„ç‰¹å¾
2. **æ•ˆç‡**: åªè®­ç»ƒåˆ†ç±»å¤´ï¼Œè®­ç»ƒé€Ÿåº¦å¿«
3. **é˜²æ­¢è¿‡æ‹Ÿåˆ**: é¿å…åœ¨å°æ•°æ®é›†ä¸Šç ´åé¢„è®­ç»ƒç‰¹å¾
4. **ä¸“æ³¨æ€§**: ä¸“é—¨é’ˆå¯¹åˆ†ç±»ä»»åŠ¡ä¼˜åŒ–

## ğŸš€ å®é™…ä½¿ç”¨å»ºè®®

1. **é¢„è®­ç»ƒé˜¶æ®µ**: ä½¿ç”¨å…¨è®­ç»ƒæ¨¡å¼å­¦ä¹ å¥½çš„è¡¨ç¤º
2. **å¾®è°ƒé˜¶æ®µ**: ä½¿ç”¨åˆ†ç±»æ¨¡å¼ï¼Œåœ¨é¢„è®­ç»ƒç‰¹å¾åŸºç¡€ä¸Šä¼˜åŒ–åˆ†ç±»æ€§èƒ½
3. **ç‰¹å¾è´¨é‡**: åˆ†ç±»æ¨¡å¼ä¾èµ–äºå…¨è®­ç»ƒæ¨¡å¼å­¦åˆ°çš„é«˜è´¨é‡ç‰¹å¾

è¿™ç§è®¾è®¡éµå¾ªäº†ç°ä»£æ·±åº¦å­¦ä¹ çš„"é¢„è®­ç»ƒ-å¾®è°ƒ"èŒƒå¼ï¼Œç¡®ä¿æ—¢èƒ½å­¦åˆ°å¥½çš„è¡¨ç¤ºï¼Œåˆèƒ½åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè·å¾—æœ€ä½³æ€§èƒ½ã€‚